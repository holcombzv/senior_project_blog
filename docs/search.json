[
  {
    "objectID": "posts/week_03-04/index.html",
    "href": "posts/week_03-04/index.html",
    "title": "Week 3-4: Model Creation",
    "section": "",
    "text": "from transformers import AutoModelForCausalLM, AutoTokenizer\nimport os\nimport torch\n\n# Load the model and tokenizer from the local directory\npretrained = \"Salesforce/codegen-350M-mono\"\ntokenizer = AutoTokenizer.from_pretrained(pretrained)\nmodel = AutoModelForCausalLM.from_pretrained(pretrained)\n\nSome weights of the model checkpoint at Salesforce/codegen-350M-mono were not used when initializing CodeGenForCausalLM: ['transformer.h.0.attn.causal_mask', 'transformer.h.1.attn.causal_mask', 'transformer.h.10.attn.causal_mask', 'transformer.h.11.attn.causal_mask', 'transformer.h.12.attn.causal_mask', 'transformer.h.13.attn.causal_mask', 'transformer.h.14.attn.causal_mask', 'transformer.h.15.attn.causal_mask', 'transformer.h.16.attn.causal_mask', 'transformer.h.17.attn.causal_mask', 'transformer.h.18.attn.causal_mask', 'transformer.h.19.attn.causal_mask', 'transformer.h.2.attn.causal_mask', 'transformer.h.3.attn.causal_mask', 'transformer.h.4.attn.causal_mask', 'transformer.h.5.attn.causal_mask', 'transformer.h.6.attn.causal_mask', 'transformer.h.7.attn.causal_mask', 'transformer.h.8.attn.causal_mask', 'transformer.h.9.attn.causal_mask']\n- This IS expected if you are initializing CodeGenForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing CodeGenForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model)."
  },
  {
    "objectID": "posts/week_03-04/index.html#model-difficulties",
    "href": "posts/week_03-04/index.html#model-difficulties",
    "title": "Week 3-4: Model Creation",
    "section": "Model Difficulties",
    "text": "Model Difficulties\nMuch of my time these weeks was spent wrestling with the model to be able to run the training on my limited hardware. The main difficulties were the limited memory of my GPU and the time it took to train. I implemented several methods to help with one or both of these issues.\n\nLayer Freezing\nOne popular technique when using transfer learning is layer freezing. When training a neural network, the weights of all of the various nodes are being adjusted based on the training data. When using a pretrained model as we are with the codegen model, it is often unnecessary to adjust all of the nodes, especially the nodes closer to the input.\n\nfrom torchinfo import summary\n\nsummary(model, input_size=(1, 128), dtypes=[torch.long])\n\n====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nCodeGenForCausalLM                                 [1, 16, 128, 64]          --\n├─CodeGenModel: 1-1                                [1, 16, 128, 64]          --\n│    └─Embedding: 2-1                              [1, 128, 1024]            52,428,800\n│    └─Dropout: 2-2                                [1, 128, 1024]            --\n│    └─ModuleList: 2-3                             --                        --\n│    │    └─CodeGenBlock: 3-1                      [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-2                      [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-3                      [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-4                      [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-5                      [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-6                      [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-7                      [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-8                      [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-9                      [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-10                     [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-11                     [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-12                     [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-13                     [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-14                     [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-15                     [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-16                     [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-17                     [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-18                     [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-19                     [1, 128, 1024]            12,590,080\n│    │    └─CodeGenBlock: 3-20                     [1, 128, 1024]            12,590,080\n│    └─LayerNorm: 2-4                              [1, 128, 1024]            2,048\n├─Linear: 1-2                                      [1, 128, 51200]           52,480,000\n====================================================================================================\nTotal params: 356,712,448\nTrainable params: 356,712,448\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 356.71\n====================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 264.24\nParams size (MB): 1426.85\nEstimated Total Size (MB): 1691.09\n====================================================================================================\n\n\nCodeGen-350M Architecture:\nCodeGenForCausalLM [1, 16, 128, 64] – ├─CodeGenModel: 1-1 [1, 16, 128, 64] – │ └─Embedding: 2-1 [1, 128, 1024] 52,428,800 │ └─Dropout: 2-2 [1, 128, 1024] – │ └─ModuleList: 2-3 – – │ │ └─CodeGenBlock: 3-1 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-2 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-3 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-4 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-5 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-6 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-7 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-8 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-9 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-10 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-11 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-12 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-13 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-14 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-15 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-16 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-17 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-18 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-19 [1, 128, 1024] 12,590,080 │ │ └─CodeGenBlock: 3-20 [1, 128, 1024] 12,590,080 │ └─LayerNorm: 2-4 [1, 128, 1024] 2,048 ├─Linear: 1-2 [1, 128, 51200] 52,480,000\nLooking at the model, we can see that after the input and dropout layers, there are 20 layers of transformation nodes. These layers will have already been tuned by the developers of the model, so we can freeze some of these layers to save us time and resources. For now, we will freeze the first half (10) transformation layers as well as the input embedding layer.\n# Freeze the embedding layer\nfor param in model.base_model.wte.parameters():\n    param.requires_grad = False\n\n# Freeze the first 10 transformer blocks\nfor layer in model.base_model.h[:10]:\n    for param in layer.parameters():\n        param.requires_grad = False\n\n\nPruning\nThis model was created to generate a variety of Python code that we don’t need for our task. This allows us to “prune” or shrink the layers to a more reasonable size. This will speed up training immensely at the potential cost of performance. For now I am pruning the transformatioon layers to half their original size, meaning that there are still 20 transformation layers, but they are all half as big as before. I may change this later if the performance is not meeting expectations.\n# Pruning\n\nimport torch.nn.utils.prune as prune\n\ndef prune_model(model, amount=0.3):\n    \"\"\"\n    Prunes the attention and MLP layers of CodeGenForCausalLM.\n    \n    Parameters:\n    - model: CodeGenForCausalLM model instance\n    - amount: Fraction of weights to prune (e.g., 0.3 means 30% of small weights are pruned)\n    \"\"\"\n    for block in model.transformer.h:\n        # Prune attention layers\n        prune.l1_unstructured(block.attn.qkv_proj, name=\"weight\", amount=amount)\n        prune.l1_unstructured(block.attn.out_proj, name=\"weight\", amount=amount)\n\n        # Prune MLP layers\n        prune.l1_unstructured(block.mlp.fc_in, name=\"weight\", amount=amount)\n        prune.l1_unstructured(block.mlp.fc_out, name=\"weight\", amount=amount)\n\nprune_model(model, amount=0.5)\n\ndef remove_pruning(model):\n    \"\"\"Removes pruning reparameterization to free up memory.\"\"\"\n    for block in model.transformer.h:\n        prune.remove(block.attn.qkv_proj, \"weight\")\n        prune.remove(block.attn.out_proj, \"weight\")\n        prune.remove(block.mlp.fc_in, \"weight\")\n        prune.remove(block.mlp.fc_out, \"weight\")\n\nremove_pruning(model)\n\n\nLoRa\nWhile researching these methods to save time and memory, I came across a technique reffered to as Low-Rank Adaptation(LoRa). LoRa is a method used during the training process of larger pretrained models. LoRa basiccally tells the trainer to prioritize lower-weight parameters. This helps the model to retain its original potency while being trained for more specific tasks. Using LoRa makes our previous modifications to the model somewhat redundant, but every little bit helps.\nApplying LoRa to our model is very straightforward. Rather than manually telling LoRa what layers to modify like we had to for freezing and pruning, LoRa automatically determines which layer parameters to modify. The main parameters that we are concerned with are r, lora_alpha, and lora_dropout. Changing these can help improve tarining time at the cost of adaptability and performance.\n# create LoRa model\nlora_config = LoraConfig(\n    r=8, # Affects size and training efficency of model. Low values(1-8) limit the size of the model during training, but model will have a harder time adapting to new tasks.\n    lora_alpha=32, # Affects the model's ability to retain previous weights. Low values (1-16) causesthe new weights to be less impactful, creating a model closer to the original.\n    lora_dropout=0.1, # Acts in a similar fashion to a dropout layer during training. Removes some of the weights for each step, preventing the model from relying too much on a single node. This is used to combat overfitting during training.\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\nmodel = get_peft_model(model, lora_config)"
  },
  {
    "objectID": "posts/week_01-02/index.html",
    "href": "posts/week_01-02/index.html",
    "title": "Week 1-2: Proposal and First Steps",
    "section": "",
    "text": "Proposal Development\nAn observation that has stuck out to me while studying and practicing data science is how innaccessible most of the tools are for things like analysis or visualization. Locked behind steep learning curves and paywalls, tools that are available to companies and research groups are not as easy to access for everyday use by people without a technical background. For my senior project, I am going to explore the potential for AI tools like natutral language processing and large language models to allow more people to explore data for everyday tasks and questions.\nThe final product will be a web app powered by a light-weight model that anyone can upload data to and ask questions about their data, similar to tools like ChatGPT. After some research, I found that tools like this are somewhat integrated into larger softwares like Tableau or Jupyter Notebook, but those require resources and experience to access and use regularly. Alongside the app, I would like to gather and report on feedback from test users.\nWith this in mind, I crafted a proposal, looking at the libraries and concepts necessary to accomplish this. This project will rely heavily on libraries from Hugging Face and TensorFlow as well as Streamlit to create the final application. The main steps towards completing the final project by the end of week 14 are:\n\nCreate the model that powers the app by converting the user’s question into basic Python code to execute on the uploaded data.\nCreate the application using Streamlit.\nDeploy the app and gather feedback.\n\n\n\nInitializing the Model\nThe first step is going to be finding and initializing a pre-developed model to start with using transfer-learning. After some research, I found a Hugging Face model called CodeParrot that looked like a great place to start. Based on GPT-2, CodeParrot is trained on thousands of code entries scraped from web-based tutorials using Python and several supporting libraries. Starting with CodeParrot, I could find additional data better suited for my use-case and train CodeParrot for use in my model.\nfrom transformers import AutoTokenizer, AutoModelWithLMHead # Necessary commands from Hugging Face's transformers library\n\npretrained = 'codeparrot/codeparrot'\ntokenizer = AutoTokenizer.from_pretrained(pretrained) # Loading the tokenizer used to pre-process the english inputs for the model.\nmodel = AutoModelWithLMHead.from_pretrained(pretrained) # Loading the actual model.\nHowever, I ran in to two immediate issues.\nFirst, the CodeParrot model isn’t compatible with TensorFlow. After trying several methods of transfering the model to TensorFlow, I decided to change my plan and use PyTorch instead going forward.\nSecond, the model is too big for my GPU’s memory. After some more research, I found another model called codegen-350M-mono from Salesforce, a lighter-weight model with the same purpose of generating Python code from English prompts.\n# Loading in pre-trained model:\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\npretrained = \"Salesforce/codegen-350M-mono\"\ntokenizer = AutoTokenizer.from_pretrained(pretrained)\nmodel = AutoModelForCausalLM.from_pretrained(pretrained)\n\n# Test of initial model:\n\nprompt = \"Write a NumPy program to repeat elements of an array.\"\ninputs = tokenizer(prompt, return_tensors=\"pt\") # Pre-process the input using the tokenizer from earlier\noutputs = model.generate(**inputs, max_length=500) # Generate the model's output.\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n# def repeat(arr, n):\n#     for i in range(n):\n#         arr[i] = arr[i] * i\n#     return arr\n\n# arr = [1, 2, 3, 4, 5]\n# n = len(arr)\n# print(repeat(arr, n))\n\n\nNext Steps\nNow that the model is loaded and working, next I will be working on finding/generating training data and tuning the model to better match our requirements."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zach Holcomb Senior Project 2025: Data Analysis AI",
    "section": "",
    "text": "Zach Holcomb Project Proposal\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWeek 1-2: Proposal and First Steps\n\n\n\n\n\n\n\ncode\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2025\n\n\nZach Holcomb\n\n\n\n\n\n\n  \n\n\n\n\nWeek 3-4: Model Creation\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 3, 2025\n\n\nZach Holcomb\n\n\n\n\n\n\n  \n\n\n\n\nWeek 1-2: Proposal and First Steps\n\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJan 21, 2025\n\n\nZach Holcomb\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/entity_embedding/index.html",
    "href": "posts/entity_embedding/index.html",
    "title": "Week 1-2: Proposal and First Steps",
    "section": "",
    "text": "What is Entity Embedding\nWhile working on my model, I was concerned about overtraining my model on the datasets I was using for training. What I realized I needed was a way to train my model in a way that the vocabulary of the model didn’t need to include the names of the columns in the dataset the model is being used to analyze. After some research, I came across a technique used for neural networks called entity embedding.\nEntity embedding is a more complicated verison of label encoding, used to solve this issue for smaller models used for categorical problems. Label encoding invloves taking categorical variables and replacing them with a set of numerical values that the model learns how to use regardless of what the values correspond to. For example, in my example, I want to encode the column names of a dataset. So, instead of passing the column name “date” to the model, label encoding would turn “date” into the value 1, for the first column. Other columns would translate to other unique values. Then, once the model runs and a result is given, the values are switched back to their categorical values. So 1 becomes “date” again.\nThis works well for simpler tree-based models. The issue with this method is that a lot of context is lot in translating each value to a single number. In our example, the model doesn’t know anything about the column 1 other than it is a column in the data. The use of numeric values can also introduce faulty relationships, such as 1 &gt; 2, that we don’t want our model to rely on. Entity embedding solves these problems by using vectors instead of single values. The model trains itself to derive meaning from each value in the vectors, solving the issues of unwanted relationships and data loss by creating purposeful relationships that allow the model to be both flexible and complex enough for larger machine learning models like neural networks.\n\n\nImplementing Entity Embedding to our Model\nTo improve the model’s understanding of the relationship between user queries and the structure of the underlying data, I implemented entity embedding for column names. This was done by creating a ‘ColumnNameEmbedder’ module that maps each unique column name to a vector. This process of translating our categorical column names to numeric vectors is refined during training. During this process, the embedded column names are put through the same pre-processing transformations as the initial query, which come from our pretrained SalesForce model codegen-350M-mono. To fuse both sources of information, I designed a function to concatenate the token-level query embeddings, or the normal input to the model, with the corresponding column name embeddings across time steps. This combined embedding is then fed into an LSTM layer, allowing the model to capture interactions between the input query and the data being referenced.\nAfter adding this, I noticed a drop in performance at the start of training. This makes sense since the model has to adjust to this new input structure. I wasn’t able to train long enough to see if this early drop leads to better results later on. In theory, this setup should improve the model’s flexibility and reduce overfitting by making it better at picking up complex patterns between queries and column names."
  },
  {
    "objectID": "posts/week_01-02/proposal.html",
    "href": "posts/week_01-02/proposal.html",
    "title": "Zach Holcomb Project Proposal",
    "section": "",
    "text": "Personal Background\nI finished MATH 425 in spring 2024 and CS 450 and MATH 488 in fall 2024. After I graduate, I hope to get a job as a strategic analyst within 3-5 years. Working anywhere as a strategic analyst is what I would like to make into my career, though I would prefer to eventually get into law enforcement or government intelligence.\nSchool email: holcombzv@byui.edu\n\n\nProject Background\nI would like to explore how to give more people access to basic data analysis using large language models. At the moment, the tools to be able to view data and use it to answer questions is locked behind expensive tools and steep learning curves. I would love to create a tool to help people without technical backgrounds use data for everyday tasks and questions.\n\n\nDomain to Investigate\n\nLLMs and Software Development\nThe main topics would be LLM training and deployment using the transformers library from Hugging Face for powerful pretrained models, TensorFlow’s models nlp library for training and deployment, Streamlit for the UI, and Hugging Face for hosting the app. For data processing, I’ll start with Polars, but I will switch to tf.data in order to integrate user-uploaded data. For training data, I can use this dataset from kaggle. I have also had some success using ChatGPT to generate more data, so I can use that as well.\n\n\n\nProposed Deliverables\nThe main deliverable would be a live app on Hugging Face that people can see and interact with. I would like to have a prototype that uses pre-processed data to answer questions. The user will be able to ask questions based on pre-defined datasets or upload their own data. I would also like to create a report on user-feedback on the live app from people both with and without technical backgrounds.\nPotential data for testing and demonstration:\n\nCovid-19 Data\nWorld Bank Economic Data\nUS Census Data (most likely a small subset)\n\n\n\nFaculty Request\nBrother Hathaway"
  }
]